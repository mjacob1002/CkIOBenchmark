
Running as 128 OS processes: ./iotest TEST_FILES/39.txt 128
charmrun> /usr/bin/setarch x86_64 -R mpirun -np 128 ./iotest TEST_FILES/39.txt 128
Charm++> Running on MPI library: Open MPI v4.0.5, package: Open MPI spack@br012.ib.bridges2.psc.edu Distribution, ident: 4.0.5, repo rev: v4.0.5, Aug 26, 2020 (MPI standard: 3.1)
Charm++> Level of thread support used: MPI_THREAD_SINGLE (desired: MPI_THREAD_SINGLE)
Charm++> Running in non-SMP mode: 128 processes (PEs)
Converse/Charm++ Commit ID: v7.1.0-devel-237-ga3f055e15
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> Running on 1 hosts (2 sockets x 64 cores x 1 PUs = 128-way SMP)
Charm++> cpu topology info is gathered in 0.035 seconds.
CharmLB> Load balancing instrumentation for communication is off.
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpirun noticed that process rank 71 with PID 25268 on node r053 exited on signal 9 (Killed).
--------------------------------------------------------------------------
2 total processes killed (some possibly by mpirun during cleanup)
slurmstepd: error: Detected 1 oom-kill event(s) in StepId=14318868.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.
